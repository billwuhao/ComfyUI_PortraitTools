import os
import torch
from copy import deepcopy
import cv2
import numpy as np
from comfy import model_management
import folder_paths
import sys
import math
from PIL import Image, ImageFont, ImageDraw, ImageEnhance, ImageChops

current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.append(current_dir)

from retinaface import RetinaFace
from layout_calculator import generate_layout_photo, generate_layout_image
from beauty import grindSkin, make_whitening, adjust_brightness_contrast_sharpen_saturation
from AILab_RMBG import (AVAILABLE_MODELS,
                        RMBGModel,
                        BENModel,
                        BEN2Model,
                        InspyrenetModel,
                        tensor2pil,
                        pil2tensor,
                        handle_model_error,
)
models_dir = folder_paths.models_dir
model_path = os.path.join(models_dir, "facedetection", "detection_Resnet50_Final.pth")
device = model_management.get_torch_device()


def init_model(half=False, device=device):
    model = RetinaFace(network_name='resnet50', device=device, half=half)
    load_net = torch.load(model_path, map_location=lambda storage, loc: storage)
    # remove unnecessary 'module.'
    for k, v in deepcopy(load_net).items():
        if k.startswith('module.'):
            load_net[k[7:]] = v
            load_net.pop(k)
    model.load_state_dict(load_net, strict=True)

    return model


def tensor_to_rgb(tensor_image):
    """
    å°†ComfyUIçš„tensorå›¾åƒè½¬æ¢ä¸ºRGBå›¾åƒ
    
    å‚æ•°:
        tensor_image: å½¢çŠ¶ä¸º[B,H,W,C]çš„tensorï¼Œé€šå¸¸ä¸ºfloat32ç±»å‹ï¼Œå€¼èŒƒå›´0-1
        
    è¿”å›:
        numpyæ•°ç»„ï¼ŒRGBæ ¼å¼ï¼Œuint8ç±»å‹ï¼Œå€¼èŒƒå›´0-255
    """
    # ComfyUIçš„tensoræ ¼å¼æ˜¯[B,H,W,C]ï¼Œå–ç¬¬ä¸€å¼ å›¾ç‰‡
    if len(tensor_image.shape) == 4:
        image = tensor_image[0].cpu().numpy()
    else:
        image = tensor_image.cpu().numpy()
    
    # è½¬æ¢ä¸º0-255èŒƒå›´çš„uint8
    image = (image * 255.0).astype(np.uint8)
    
    return image


def rgb_to_tensor(image):
    """
    å°†RGBå›¾åƒè½¬æ¢å›ComfyUIçš„tensoræ ¼å¼
    
    å‚æ•°:
        image: numpyæ•°ç»„ï¼ŒRGBæ ¼å¼ï¼Œuint8ç±»å‹
        
    è¿”å›:
        å½¢çŠ¶ä¸º[1,H,W,C]çš„tensorï¼Œfloat32ç±»å‹ï¼Œå€¼èŒƒå›´0-1
    """
    # è½¬æ¢ä¸ºfloat32å¹¶å½’ä¸€åŒ–åˆ°0-1
    image = image.astype(np.float32) / 255.0
    
    # è½¬æ¢ä¸ºtensorå¹¶æ·»åŠ æ‰¹æ¬¡ç»´åº¦
    image = torch.from_numpy(image).unsqueeze(0)
    
    return image


class Watermarker(object):
    """å›¾ç‰‡æ°´å°å·¥å…·"""

    def __init__(
        self,
        input_image: Image.Image,
        text: str,
        font_file: str,
        angle=30,
        color="#8B8B1B",
        opacity=0.15,
        size=50,
        space=75,
        chars_per_line=8,
        font_height_crop=1.2,
    ):
        """_summary_

        Parameters
        ----------
        input_image : Image.Image
            PILå›¾ç‰‡å¯¹è±¡
        text : str
            æ°´å°æ–‡å­—
        angle : int, optional
            æ°´å°è§’åº¦, by default 30
        color : str, optional
            æ°´å°é¢œè‰², by default "#8B8B1B"
        font_file : str, optional
            å­—ä½“æ–‡ä»¶, by default "é’é¸Ÿåå…‰ç®€ç¥ç€.ttf"
        font_height_crop : float, optional
            å­—ä½“é«˜åº¦è£å‰ªæ¯”ä¾‹, by default 1.2
        opacity : float, optional
            æ°´å°é€æ˜åº¦, by default 0.15
        size : int, optional
            å­—ä½“å¤§å°, by default 50
        space : int, optional
            æ°´å°é—´è·, by default 75
        chars_per_line : int, optional
            æ¯è¡Œå­—ç¬¦æ•°, by default 8
        """
        self.input_image = input_image
        self.text = text
        self.angle = angle
        self.color = color
        self.font_file = font_file
        self.font_height_crop = font_height_crop
        self.opacity = opacity
        self.size = size
        self.space = space
        self.chars_per_line = chars_per_line
        self.image = self._add_mark_striped()

    @staticmethod
    def set_image_opacity(image, opacity: float):
        alpha = image.split()[3]
        alpha = ImageEnhance.Brightness(alpha).enhance(opacity)
        image.putalpha(alpha)
        return image

    @staticmethod
    def crop_image_edge(image):
        bg = Image.new(mode="RGBA", size=image.size)
        diff = ImageChops.difference(image, bg)
        bbox = diff.getbbox()
        if bbox:
            return image.crop(bbox)
        return image

    def _add_mark_striped(self):
        origin_image = self.input_image.convert("RGBA")
        width = len(self.text) * self.size
        height = round(self.size * self.font_height_crop)
        watermark_image = Image.new(mode="RGBA", size=(width, height))
        draw_table = ImageDraw.Draw(watermark_image)
        draw_table.text(
            (0, 0),
            self.text,
            fill=self.color,
            font=ImageFont.truetype(self.font_file, size=self.size),
        )
        watermark_image = Watermarker.crop_image_edge(watermark_image)
        Watermarker.set_image_opacity(watermark_image, self.opacity)

        c = int(math.sqrt(origin_image.size[0] ** 2 + origin_image.size[1] ** 2))
        watermark_mask = Image.new(mode="RGBA", size=(c, c))
        y, idx = 0, 0
        while y < c:
            x = -int((watermark_image.size[0] + self.space) * 0.5 * idx)
            idx = (idx + 1) % 2
            while x < c:
                watermark_mask.paste(watermark_image, (x, y))
                x += watermark_image.size[0] + self.space
            y += watermark_image.size[1] + self.space

        watermark_mask = watermark_mask.rotate(self.angle)
        origin_image.paste(
            watermark_mask,
            (int((origin_image.size[0] - c) / 2), int((origin_image.size[1] - c) / 2)),
            mask=watermark_mask.split()[3],
        )
        return origin_image


class ImageWatermark:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "image": ("IMAGE",),
                "text": ("STRING", {"default": "@æ˜æ–‡è§†ç•Œ"}),
                "font_file": ("STRING", {"default": ""}),
                "angle": ("INT", {"default": 30, "min": 0, "max": 360, "step": 1}),
                "red": ("INT", {"default": 0, "min": 0, "max": 255, "step": 1}),
                "green": ("INT", {"default": 0, "min": 0, "max": 255, "step": 1}),
                "blue": ("INT", {"default": 0, "min": 0, "max": 255, "step": 1}),
                "opacity": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 1.0, "step": 0.1}),
                "size": ("INT", {"default": 50, "min": 1, "max": 100, "step": 1}),
                "space": ("INT", {"default": 75, "min": 1, "max": 150, "step": 1}),
                # "chars_per_line": ("INT", {"default": 8, "min": 1, "max": 10, "step": 1}),
                # "font_height_crop": ("FLOAT", {"default": 1.2, "min": 0.1, "max": 5.0, "step": 0.1})
            },
        }

    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("image",)
    FUNCTION = "watermarkgen"
    CATEGORY = "ğŸ¤MW/MW-PortraitTools"

    def watermarkgen(
        self,
        image,
        text: str,
        font_file: str,
        angle=30,
        red=0,
        green=0,
        blue=0,
        opacity=0.15,
        size=50,
        space=75,
        chars_per_line=8,
        font_height_crop=1.2,
    ):
        if font_file.strip() == "":
            font_file = os.path.join(current_dir, "ChironGoRoundTC-600SB.ttf")
        watermarker = Watermarker(
            input_image=tensor2pil(image),
            text=text,
            font_file=font_file,
            angle=angle,
            color=f"#{red:02x}{green:02x}{blue:02x}",
            opacity=opacity,
            size=size,
            space=space,
            chars_per_line=chars_per_line,
            font_height_crop=font_height_crop,
        )

        return (pil2tensor(watermarker.image),)

class AlignFace:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "image": ("IMAGE",),
                "half":  ("BOOLEAN", {"default": False}),
                "angle_offset": ("FLOAT", {"default": 1.0, "min": -180.0, "max": 180.0, "step": 0.1}),
                # "unload_model": ("BOOLEAN", {"default": False}),
            },
        }

    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("image",)
    FUNCTION = "detect_and_align_whole_image"
    CATEGORY = "ğŸ¤MW/MW-PortraitTools"

    def detect_and_align_whole_image(self,
                                image, 
                                half, 
                                angle_offset=1.0,
                                conf_threshold=0.8,
                                nms_threshold=0.4, 
                                use_origin_size=True):
        # åˆå§‹åŒ–æ¨¡å‹
        face_detector = init_model(half=half, device=device)
        
        # è¯»å–å›¾åƒ
        img_rgb = tensor_to_rgb(image)
        
        # æ£€æµ‹äººè„¸
        face_info = face_detector.detect_faces(img_rgb, 
                                conf_threshold=conf_threshold, 
                                nms_threshold=nms_threshold, 
                                use_origin_size=use_origin_size
                                )

        if len(face_info) == 0:
            print("æœªæ£€æµ‹åˆ°äººè„¸")
            return (image,)
        
        # è·å–æœ€å¤§çš„äººè„¸ï¼ˆå‡è®¾ä¸»è¦äººè„¸æ˜¯æœ€å¤§çš„ï¼‰
        areas = (face_info[:, 2] - face_info[:, 0]) * (face_info[:, 3] - face_info[:, 1])
        max_face_idx = np.argmax(areas)
        face_box = face_info[max_face_idx, 0:4]
        landmarks = face_info[max_face_idx, 5:15].reshape(5, 2)
        
        # æå–å…³é”®ç‚¹
        facial5points = [[landmarks[j][0], landmarks[j][1]] for j in range(5)]
        
        # ä½¿ç”¨ç®€å•çš„æ–¹æ³•è¿›è¡Œå¯¹é½
        # è®¡ç®—çœ¼ç›ä¸­å¿ƒç‚¹ï¼ˆä½¿ç”¨å‰ä¸¤ä¸ªå…³é”®ç‚¹ï¼Œå®ƒä»¬é€šå¸¸æ˜¯å·¦å³çœ¼ï¼‰
        left_eye = facial5points[0]
        right_eye = facial5points[1]
        
        # è®¡ç®—çœ¼ç›ä¹‹é—´çš„è§’åº¦
        dy = right_eye[1] - left_eye[1]
        dx = right_eye[0] - left_eye[0]
        angle = np.degrees(np.arctan2(dy, dx))
        
        # è®¡ç®—çœ¼ç›ä¸­å¿ƒ
        eye_center = ((left_eye[0] + right_eye[0]) // 2, (left_eye[1] + right_eye[1]) // 2)
        
        # è·å–æ—‹è½¬çŸ©é˜µ
        M = cv2.getRotationMatrix2D(eye_center, angle + angle_offset, 1)
        
        # å¯¹æ•´ä¸ªå›¾åƒè¿›è¡Œæ—‹è½¬
        h, w = img_rgb.shape[:2]
        aligned_image = cv2.warpAffine(img_rgb, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)
        image_tensor = rgb_to_tensor(aligned_image)

        # é‡æ–°æ£€æµ‹æ—‹è½¬åçš„äººè„¸ï¼Œä»¥è·å–æ–°çš„è¾¹ç•Œæ¡†
        rotated_face_info = face_detector.detect_faces(aligned_image, 
                                conf_threshold=conf_threshold, 
                                nms_threshold=nms_threshold, 
                                use_origin_size=use_origin_size
                                )
        
        if len(rotated_face_info) == 0:
            print("å¯¹é½åæœªæ£€æµ‹åˆ°äººè„¸ï¼Œè¿”å›åŸå§‹å›¾åƒ")
            return (image,)
        else:  
            return (image_tensor,)


class DetectCropFaces:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "image": ("IMAGE",),
                "half":  ("BOOLEAN", {"default": False}),
                "horizontal_padding": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 5.0, "step": 0.1}),
                "vertical_padding": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 5.0, "step": 0.1}),
                "do_align": ("BOOLEAN", {"default": True}),
                "angle_offset": ("FLOAT", {"default": 1.0, "min": -10.0, "max": 10.0, "step": 0.1}),
                # "unload_model": ("BOOLEAN", {"default": False}),
            },
        }

    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("image",)
    FUNCTION = "detect_and_align_faces"
    CATEGORY = "ğŸ¤MW/MW-PortraitTools"

    def detect_and_align_faces(self,
                                image, 
                                half, 
                                horizontal_padding,
                                vertical_padding,
                                do_align=True, 
                                angle_offset=0.1,
                                conf_threshold=0.8,
                                nms_threshold=0.4, 
                                use_origin_size=True):
        # åˆå§‹åŒ–æ¨¡å‹
        face_detector = init_model(half=half, device=device)
        
        # è¯»å–å›¾åƒ
        img_rgb = tensor_to_rgb(image)

        _, aligned_faces = face_detector.align_multi(
                                img_rgb, 
                                padding=(horizontal_padding, vertical_padding), 
                                do_align=do_align, 
                                angle_offset=angle_offset,
                                conf_threshold=conf_threshold, 
                                nms_threshold=nms_threshold, 
                                use_origin_size=use_origin_size,
                                limit=None)

        if len(aligned_faces) == 0:
            print("æ²¡æœ‰æ£€æµ‹åˆ°äººè„¸ï¼Œè¿”å›åŸå§‹å›¾åƒ")
            return (image,)

        # å¦‚æœåªæ£€æµ‹åˆ°ä¸€ä¸ªäººè„¸ï¼Œç›´æ¥è¿”å›
        if len(aligned_faces) == 1:
            face_tensor = rgb_to_tensor(aligned_faces[0])
            return (face_tensor,)
        
        face_tensors = []
        # æ‰¾å‡ºæ‰€æœ‰äººè„¸ä¸­æœ€å¤§çš„å°ºå¯¸
        max_h = max([face.shape[0] for face in aligned_faces])
        max_w = max([face.shape[1] for face in aligned_faces])
        
        for i, face in enumerate(aligned_faces):
            # è°ƒæ•´æ‰€æœ‰äººè„¸åˆ°ç›¸åŒå¤§å°
            resized_face = cv2.resize(face, (max_w, max_h), interpolation=cv2.INTER_CUBIC)
            face_tensor = rgb_to_tensor(resized_face)
            face_tensors.append(face_tensor)

        # å°†æ‰€æœ‰äººè„¸tensoræ‹¼æ¥æˆä¸€ä¸ªæ‰¹æ¬¡
        batch_tensor = torch.cat(face_tensors, dim=0)
        
        # è¿”å›æ‰¹æ¬¡tensor
        return (batch_tensor,)


size_list = [
    "ä¸€å¯¸,413,295",
    "äºŒå¯¸,626,413",
    "å°ä¸€å¯¸,378,260",
    "å°äºŒå¯¸,531,413",
    "å¤§ä¸€å¯¸,567,390",
    "å¤§äºŒå¯¸,626,413",
    "äº”å¯¸,1499,1050",
    "æ•™å¸ˆèµ„æ ¼è¯,413,295",
    "å›½å®¶å…¬åŠ¡å‘˜è€ƒè¯•,413,295",
    "åˆçº§ä¼šè®¡è€ƒè¯•,413,295",
    "è‹±è¯­å››å…­çº§è€ƒè¯•,192,144",
    "è®¡ç®—æœºç­‰çº§è€ƒè¯•,567,390",
    "ç ”ç©¶ç”Ÿè€ƒè¯•,709,531",
    "ç¤¾ä¿å¡,441,358",
    "ç”µå­é©¾é©¶è¯,378,260",
    "ç¾å›½ç­¾è¯,600,600",
    "æ—¥æœ¬ç­¾è¯,413,295",
    "éŸ©å›½ç­¾è¯,531,413" 
]

bg_colors = {
    "Alpha": None,
    "black": (0, 0, 0),             
    "white": (255, 255, 255),      
    "gray": (128, 128, 128),      
    "green": (0, 255, 0),         
    "pure_blue": (0, 0, 255),    
    "pure_red": (255, 0, 0),      
    "cornflower_blue": (98, 139, 206),  
    "crimson_red": (215, 69, 50),      
    "dark_slate_blue": (75, 97, 144),  
    "snow_white": (242, 240, 240)  
}

class IDPhotos:
    def __init__(self):
        self.models = {
            "RMBG-2.0": RMBGModel(),
            "INSPYRENET": InspyrenetModel(),
            "BEN": BENModel(),
            "BEN2": BEN2Model()
        }
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required":{
                "image":("IMAGE",),
                "rmbg_model":(list(AVAILABLE_MODELS),{"default":"RMBG-2.0"}),
                "bg_color":(list(bg_colors.keys()),{"default":"Alpha"}),
                "size":(size_list,{"default":"ä¸€å¯¸,413,295"}),
                "kb":("INT",{"default":500,"min":5,"max":2000,"step":1}),
                "dpi":("INT",{"default":300,"min":50,"max":1000,"step":10}),
                "face_reduction":("FLOAT",{
                    "default": 1.0,
                    "min":0.0,
                    "max":5.0,
                    "step":0.1,
                }),
                "face_up_down":("FLOAT",{
                    "default": 0.0,
                    "min":-0.5,
                    "max":0.5,
                    "step":0.01,
                }),
                "angle_offset":("FLOAT",{
                    "default": 1.0,
                    "min":-10.0,
                    "max":10.0,
                    "step":0.1,
                }),
            }
        }
    
    RETURN_TYPES = ("IMAGE", "IMAGE", "IMAGE")
    RETURN_NAMES = ("standard_photo", "hd_photo", "print_photos")
    FUNCTION = "gen_img"
    CATEGORY = "ğŸ¤MW/MW-PortraitTools"
        
    def gen_img(self, image, rmbg_model, bg_color, size, face_reduction, face_up_down, angle_offset, kb, dpi=300):
        # è§£æå°ºå¯¸å‚æ•°
        size_parts = size.split(',')
        size = (int(size_parts[1]), int(size_parts[2])) 

        hd_photo = self.photo_gen(image, size, face_reduction, face_up_down, angle_offset, by_size=False)
        rmbg_hd_photo = self.image_rmbg(hd_photo, rmbg_model, bg_color)
        standard_photo = self.photo_gen(image, size, face_reduction, face_up_down, angle_offset, by_size=True)
        rmbg_standard_photo = self.image_rmbg(standard_photo, rmbg_model, bg_color)
        print_photos = self.print_photos_gen(rmbg_standard_photo, size, kb, dpi)

        return (rmbg_standard_photo, rmbg_hd_photo, print_photos)
    
    def photo_gen(self, image, size, face_reduction, face_up_down, angle_offset, by_size=True):
        # åˆå§‹åŒ–æ¨¡å‹
        face_detector = init_model(half=True, device=device)
        
        # è¯»å–å›¾åƒ
        img_rgb = tensor_to_rgb(image)

        _, aligned_faces = face_detector.align_multi(
                                img_rgb, 
                                angle_offset=angle_offset,
                                padding=(face_reduction, face_reduction), 
                                do_align=True, 
                                conf_threshold=0.8, 
                                nms_threshold=0.4, 
                                use_origin_size=True,
                                limit=None)

        if len(aligned_faces) > 1:
            raise ValueError("Multiple faces detected, please upload an image of a single face.")
        if len(aligned_faces) == 0:
            raise ValueError("No face detected, please upload an image containing the face.")

        face = aligned_faces[0]
        
        # è·å–ç›®æ ‡å°ºå¯¸
        target_h = size[0]
        target_w = size[1]
        
        # è·å–äººè„¸å›¾åƒçš„å°ºå¯¸
        face_h, face_w = face.shape[:2]
        
        if by_size:
            # æ¨¡å¼1: æŒ‰æŒ‡å®šå°ºå¯¸è°ƒæ•´
            # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹ï¼Œä½¿ç…§ç‰‡é«˜åº¦æ¯”ç›®æ ‡é«˜åº¦å¤š target_h * face_up_down
            scale_h = (target_h + target_h * abs(face_up_down)) / face_h
            scaled_w = int(face_w * scale_h)
            scaled_h = int(face_h * scale_h)
            # å¦‚æœå®½åº¦å°äºç›®æ ‡å®½åº¦ï¼Œç»§ç»­æ”¾å¤§
            if scaled_w < target_w:
                scale_w = target_w / scaled_w
                scaled_w = target_w
                scaled_h = int(scaled_h * scale_w)
            
            # ç¼©æ”¾å›¾åƒ
            scaled_face = cv2.resize(face, (scaled_w, scaled_h), interpolation=cv2.INTER_LANCZOS4)
            
            # è®¡ç®—è£å‰ªåŒºåŸŸ
            center_x = scaled_w // 2
            # ç§»åŠ¨è£å‰ªåŒºåŸŸ
            center_y = int(scaled_h // 2 + target_h * face_up_down / 2)
            
            # è®¡ç®—è£å‰ªåŒºåŸŸçš„å·¦ä¸Šè§’å’Œå³ä¸‹è§’åæ ‡
            left = center_x - target_w // 2
            right = left + target_w
            top = center_y - target_h // 2
            bottom = top + target_h
            
            # ç¡®ä¿è£å‰ªåŒºåŸŸåœ¨å›¾åƒå†…
            if left < 0:
                left = 0
                right = target_w
            if right > scaled_w:
                right = scaled_w
                left = scaled_w - target_w
            if top < 0:
                top = 0
                bottom = target_h
            if bottom > scaled_h:
                bottom = scaled_h
                top = scaled_h - target_h
            
            # è£å‰ªå›¾åƒ
            final_image = scaled_face[top:bottom, left:right]
            
            # ç¡®ä¿æœ€ç»ˆå›¾åƒå°ºå¯¸æ­£ç¡®
            if final_image.shape[:2] != (target_h, target_w):
                final_image = cv2.resize(final_image, (target_w, target_h), interpolation=cv2.INTER_LANCZOS4)
        else:
            # æ¨¡å¼2: ä¸¥æ ¼æŒ‰æ¯”ä¾‹è°ƒæ•´å°ºå¯¸ï¼Œä¸ç¼©æ”¾ç…§ç‰‡
            # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹ï¼Œä½¿ç…§ç‰‡é«˜åº¦æ¯”ç›®æ ‡é«˜åº¦å¤š target_h * face_up_down
            adjusted_target_h = int(face_h/(1 + abs(face_up_down)))
            adjusted_target_w = int(target_w * (adjusted_target_h / target_h))
            
            # å¦‚æœè°ƒæ•´åçš„å®½åº¦è¶…è¿‡äº†ç…§ç‰‡å®½åº¦ï¼Œéœ€è¦é‡æ–°è®¡ç®—
            if adjusted_target_w > face_w:
                # æŒ‰ç…§å®½åº¦è®¡ç®—æ¯”ä¾‹
                scale_w = face_w / target_w
                adjusted_target_w = face_w
                adjusted_target_h = int(target_h * scale_w)
            
            # è®¡ç®—è£å‰ªåŒºåŸŸ
            center_x = face_w // 2
            # ç§»åŠ¨è£å‰ªåŒºåŸŸ
            center_y = int(face_h // 2 + adjusted_target_h * face_up_down / 2)
            
            # è®¡ç®—è£å‰ªåŒºåŸŸçš„å·¦ä¸Šè§’å’Œå³ä¸‹è§’åæ ‡
            left = center_x - adjusted_target_w // 2
            right = left + adjusted_target_w
            top = center_y - adjusted_target_h // 2
            bottom = top + adjusted_target_h
            
            # ç¡®ä¿è£å‰ªåŒºåŸŸåœ¨å›¾åƒå†…
            if left < 0:
                left = 0
                right = adjusted_target_w
            if right > face_w:
                right = face_w
                left = face_w - adjusted_target_w
            if top < 0:
                top = 0
                bottom = adjusted_target_h
            if bottom > face_h:
                bottom = face_h
                top = face_h - adjusted_target_h
            
            # è£å‰ªå›¾åƒ
            final_image = face[top:bottom, left:right]

        # è½¬æ¢å›tensoræ ¼å¼
        result_tensor = rgb_to_tensor(final_image)
        
        return result_tensor

    def print_photos_gen(self, input_image, size, kb, dpi=300):
        # å°†tensorè½¬æ¢ä¸ºRGBå›¾åƒ
        img_rgb = tensor_to_rgb(input_image)

        from io import BytesIO
        pil_img = Image.fromarray(img_rgb)
        
        # åˆ›å»ºå­—èŠ‚æµå¯¹è±¡
        img_byte_arr = BytesIO()
        
        # ä¿å­˜åˆ°å­—èŠ‚æµ
        pil_img.save(img_byte_arr, format="PNG", dpi=(dpi, dpi))
        img_byte_arr.seek(0)

        # è°ƒæ•´å›¾åƒå¤§å°åˆ°æŒ‡å®šKB
        quality = 95
        while True:
            # åˆ›å»ºå­—èŠ‚æµå¯¹è±¡
            img_byte_arr = BytesIO()
            
            # ä¿å­˜å›¾åƒåˆ°å­—èŠ‚æµ
            pil_img.save(img_byte_arr, format="PNG", quality=quality, dpi=(dpi, dpi))
            
            # è·å–å›¾åƒå¤§å°(KB)
            img_size_kb = len(img_byte_arr.getvalue()) / 1024
            
            # æ£€æŸ¥å›¾åƒå¤§å°æ˜¯å¦åœ¨ç›®æ ‡èŒƒå›´å†…
            if img_size_kb <= kb or quality == 1:
                # å¦‚æœå›¾åƒå°äºç›®æ ‡å¤§å°ï¼Œæ·»åŠ å¡«å……
                if img_size_kb < kb:
                    padding_size = int(
                        (kb * 1024) - len(img_byte_arr.getvalue())
                    )
                    padding = b"\x00" * padding_size
                    img_byte_arr.write(padding)
                
                break
            
            # å¦‚æœå›¾åƒä»ç„¶å¤ªå¤§ï¼Œé™ä½è´¨é‡
            quality -= 5
            
            # ç¡®ä¿è´¨é‡ä¸ä½äº1
            if quality < 1:
                quality = 1
        
        # å°†å­—èŠ‚æµè½¬æ¢å›PILå›¾åƒ
        img_byte_arr.seek(0)
        pil_img = Image.open(img_byte_arr)
        
        result_layout_photo = cv2.cvtColor(np.array(pil_img), cv2.COLOR_BGR2RGB)
        
        # ç”Ÿæˆå¸ƒå±€
        typography_arr, typography_rotate = generate_layout_photo(
            input_height=size[0], input_width=size[1]
        )
        
        # ç”Ÿæˆæœ€ç»ˆå¸ƒå±€å›¾åƒ
        result_layout_image = generate_layout_image(
            result_layout_photo,
            typography_arr,
            typography_rotate,
            height=size[0],
            width=size[1],
        )
        
        # è½¬æ¢ä¸ºRGBå¹¶è½¬æ¢ä¸ºtensor
        print_cv2 = cv2.cvtColor(result_layout_image, cv2.COLOR_BGR2RGB)
        print_photos = rgb_to_tensor(print_cv2)
        
        return print_photos


    def image_rmbg(self, image, model, bg_color):
        model_instance = self.models[model]
        params = {
            "sensitivity": 1.0,
            "process_res": 1024,
            "mask_blur": 0,
            "mask_offset": 0,
            "background": bg_colors[bg_color],
            "invert_output": False,
            "optimize": "default",
            "refine_foreground": False
        }
        # Check and download model if needed
        cache_status, message = model_instance.check_model_cache(model)
        if not cache_status:
            print(f"Cache check: {message}")
            print("Downloading required model files...")
            download_status, download_message = model_instance.download_model(model)
            if not download_status:
                handle_model_error(download_message)
            print("Model files downloaded successfully")
        
        # Get mask from specific model
        mask = model_instance.process_image(image, model, params)

        # Ensure mask is in the correct format
        if isinstance(mask, list):
            masks = [m.convert("L") for m in mask if isinstance(m, Image.Image)]
            mask = masks[0] if masks else None
        elif isinstance(mask, Image.Image):
            mask = mask.convert("L")

        # Post-process mask
        mask_tensor = pil2tensor(mask)
        mask_tensor = mask_tensor * (1 + (1 - params["sensitivity"]))
        mask_tensor = torch.clamp(mask_tensor, 0, 1)
        mask = tensor2pil(mask_tensor)
        
        # Create final image
        orig_image = tensor2pil(image)

        orig_rgba = orig_image.convert("RGBA")
        r, g, b, _ = orig_rgba.split()
        foreground = Image.merge('RGBA', (r, g, b, mask))

        if bg_color != "Alpha":
            bg_color = bg_colors[bg_color]
            bg_image = Image.new('RGBA', orig_image.size, (*bg_color, 255))
            composite_image = Image.alpha_composite(bg_image, foreground)
            processed_image = pil2tensor(composite_image.convert("RGB"))
        else:
            processed_image = pil2tensor(foreground)
        
        return processed_image
        

class BeautifyPhoto:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required":{
                "image":("IMAGE",),
                "whitening_strength":("INT",{
                    "default": 0,
                    "min":0,
                    "max":100,
                    "step":1,
                }),
                 "brightness_strength":("INT",{
                    "default": 0,
                    "min":-100,
                    "max":100,
                    "step":1,
                }),
                "contrast_strength":("INT",{
                    "default": 0,
                    "min":-100,
                    "max":100,
                    "step":1,
                }),
                "saturation_strength":("INT",{
                    "default": 0,
                    "min":-100,
                    "max":100,
                    "step":1,
                }),
                "sharpen_strength":("FLOAT",{
                    "default": 0.1,
                    "min":0.0,
                    "max":10.0,
                    "step":0.1,
                }),
                "grind_skin":("INT",{
                    "default": 0,
                    "min":0,
                    "max":10,
                    "step":1,
                }),
            }
        }
    
    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("image",)
    FUNCTION = "beautify"
    CATEGORY = "ğŸ¤MW/MW-PortraitTools"
        
    def beautify(self,
                image,
                whitening_strength,
                brightness_strength,
                contrast_strength,
                saturation_strength,
                sharpen_strength,
                grind_skin,
                ):
        
        img_np = tensor_to_rgb(image)
        input_image = cv2.cvtColor(img_np,cv2.COLOR_BGR2RGB)
        
        adjusted_image = grindSkin(input_image, strength=grind_skin)
        adjusted_image = make_whitening(adjusted_image, strength=whitening_strength)
        adjusted_image = adjust_brightness_contrast_sharpen_saturation(
            adjusted_image, 
            brightness_factor=brightness_strength,
            contrast_factor=contrast_strength,
            sharpen_strength=sharpen_strength,
            saturation_factor=saturation_strength,
            )

        result_image = cv2.cvtColor(adjusted_image,cv2.COLOR_BGR2RGB)
        result_tensor = rgb_to_tensor(result_image)

        return (result_tensor,)


NODE_CLASS_MAPPINGS = {
    "DetectCropFace": DetectCropFaces,
    "AlignFace": AlignFace,
    "IDPhotos": IDPhotos,
    "BeautifyPhoto": BeautifyPhoto,
    "ImageWatermark": ImageWatermark,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "DetectCropFaces": "Detect and Crop Faces",
    "AlignFace": "Align Face",
    "IDPhotos": "ID Photos",
    "BeautifyPhoto": "Beautify Photo",
    "ImageWatermark": "Image Watermark",
}