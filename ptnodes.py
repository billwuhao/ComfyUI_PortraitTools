import os
import torch
from copy import deepcopy
import cv2
import numpy as np
from comfy import model_management
import folder_paths
import sys
import math
from typing import List, Optional, Union
from PIL import Image, ImageFont, ImageDraw, ImageEnhance, ImageChops, ImageOps, ImageSequence

current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.append(current_dir)

from .retinaface import RetinaFace
from .layout_calculator import generate_layout_photo, generate_layout_image
from .beauty import grindSkin, make_whitening, adjust_brightness_contrast_sharpen_saturation
from .AI1038Lab_RMBG import (AVAILABLE_MODELS,
                        RMBGModel,
                        BENModel,
                        BEN2Model,
                        InspyrenetModel,
                        tensor2pil,
                        pil2tensor,
                        handle_model_error,
)

models_dir = folder_paths.models_dir
model_path = os.path.join(models_dir, "facedetection", "detection_Resnet50_Final.pth")
device = model_management.get_torch_device()


def init_model(half=False, device=device):
    model = RetinaFace(network_name='resnet50', device=device, half=half)
    load_net = torch.load(model_path, map_location=lambda storage, loc: storage)
    # remove unnecessary 'module.'
    for k, v in deepcopy(load_net).items():
        if k.startswith('module.'):
            load_net[k[7:]] = v
            load_net.pop(k)
    model.load_state_dict(load_net, strict=True)

    return model


def tensor_to_rgb(tensor_image):
    """
    å°†ComfyUIçš„tensorå›¾åƒè½¬æ¢ä¸ºRGBå›¾åƒ
    
    å‚æ•°:
        tensor_image: å½¢çŠ¶ä¸º[B,H,W,C]çš„tensorï¼Œé€šå¸¸ä¸ºfloat32ç±»å‹ï¼Œå€¼èŒƒå›´0-1
        
    è¿”å›:
        numpyæ•°ç»„ï¼ŒRGBæ ¼å¼ï¼Œuint8ç±»å‹ï¼Œå€¼èŒƒå›´0-255
    """
    # ComfyUIçš„tensoræ ¼å¼æ˜¯[B,H,W,C]ï¼Œå–ç¬¬ä¸€å¼ å›¾ç‰‡
    if len(tensor_image.shape) == 4:
        image = tensor_image[0].cpu().numpy()
    else:
        image = tensor_image.cpu().numpy()
    
    # è½¬æ¢ä¸º0-255èŒƒå›´çš„uint8
    image = (image * 255.0).astype(np.uint8)
    
    return image


def rgb_to_tensor(image):
    """
    å°†RGBå›¾åƒè½¬æ¢å›ComfyUIçš„tensoræ ¼å¼
    
    å‚æ•°:
        image: numpyæ•°ç»„ï¼ŒRGBæ ¼å¼ï¼Œuint8ç±»å‹
        
    è¿”å›:
        å½¢çŠ¶ä¸º[1,H,W,C]çš„tensorï¼Œfloat32ç±»å‹ï¼Œå€¼èŒƒå›´0-1
    """
    # è½¬æ¢ä¸ºfloat32å¹¶å½’ä¸€åŒ–åˆ°0-1
    image = image.astype(np.float32) / 255.0
    
    # è½¬æ¢ä¸ºtensorå¹¶æ·»åŠ æ‰¹æ¬¡ç»´åº¦
    image = torch.from_numpy(image).unsqueeze(0)
    
    return image


class Watermarker(object):
    """å›¾ç‰‡æ°´å°å·¥å…·"""

    def __init__(
        self,
        input_image: Image.Image,
        text: str,
        font_file: str,
        angle=30,
        color="#8B8B1B",
        opacity=0.15,
        size=50,
        space=75,
        chars_per_line=8,
        font_height_crop=1.2,
        offset_x=0,
        offset_y=0,
    ):
        """_summary_

        Parameters
        ----------
        input_image : Image.Image
            PILå›¾ç‰‡å¯¹è±¡
        text : str
            æ°´å°æ–‡å­—
        angle : int, optional
            æ°´å°è§’åº¦, by default 30
        color : str, optional
            æ°´å°é¢œè‰², by default "#8B8B1B"
        font_file : str, optional
            å­—ä½“æ–‡ä»¶, by default "é’é¸Ÿåå…‰ç®€ç¥ç€.ttf"
        font_height_crop : float, optional
            å­—ä½“é«˜åº¦è£å‰ªæ¯”ä¾‹, by default 1.2
        opacity : float, optional
            æ°´å°é€æ˜åº¦, by default 0.15
        size : int, optional
            å­—ä½“å¤§å°, by default 50
        space : int, optional
            æ°´å°é—´è·, by default 75
        chars_per_line : int, optional
            æ¯è¡Œå­—ç¬¦æ•°, by default 8
        """
        self.input_image = input_image
        self.text = text
        self.angle = angle
        self.color = color
        self.font_file = font_file
        self.font_height_crop = font_height_crop
        self.opacity = opacity
        self.size = size
        self.space = space
        self.chars_per_line = chars_per_line
        self.offset_x = offset_x
        self.offset_y = offset_y
        self.image = self._add_mark_striped()

    @staticmethod
    def set_image_opacity(image, opacity: float):
        alpha = image.split()[3]
        alpha = ImageEnhance.Brightness(alpha).enhance(opacity)
        image.putalpha(alpha)
        return image

    @staticmethod
    def crop_image_edge(image):
        bg = Image.new(mode="RGBA", size=image.size)
        diff = ImageChops.difference(image, bg)
        bbox = diff.getbbox()
        if bbox:
            return image.crop(bbox)
        return image

    def _add_mark_striped(self):
        origin_image = self.input_image.convert("RGBA")
        width = len(self.text) * self.size
        height = round(self.size * self.font_height_crop)
        watermark_image = Image.new(mode="RGBA", size=(width, height))
        draw_table = ImageDraw.Draw(watermark_image)
        draw_table.text(
            (0, 0),
            self.text,
            fill=self.color,
            font=ImageFont.truetype(self.font_file, size=self.size),
        )
        watermark_image = Watermarker.crop_image_edge(watermark_image)
        Watermarker.set_image_opacity(watermark_image, self.opacity)

        # ç¡®ä¿æ°´å°è¦†ç›–æ•´ä¸ªå›¾åƒï¼Œå¢åŠ æ°´å°æ©ç çš„å°ºå¯¸
        c = int(math.sqrt(origin_image.size[0] ** 2 + origin_image.size[1] ** 2) * 1.5)
        watermark_mask = Image.new(mode="RGBA", size=(c, c))
        
        y, idx = 0, 0
        while y < c:
            x = -int((watermark_image.size[0] + self.space) * 0.5 * idx)
            idx = (idx + 1) % 2
            while x < c:
                watermark_mask.paste(watermark_image, (x, y))
                x += watermark_image.size[0] + self.space
            y += watermark_image.size[1] + self.space

        watermark_mask = watermark_mask.rotate(self.angle)
        # è®¡ç®—å®‰å…¨çš„åç§»èŒƒå›´
        max_offset = c // 4  # é™åˆ¶æœ€å¤§åç§»é‡ä¸ºæ°´å°æ©ç å°ºå¯¸çš„1/4
        safe_offset_x = max(min(self.offset_x, max_offset), -max_offset)
        safe_offset_y = max(min(self.offset_y, max_offset), -max_offset)
        
        paste_x = int((origin_image.size[0] - c) / 2) + safe_offset_x
        paste_y = int((origin_image.size[1] - c) / 2) + safe_offset_y
        origin_image.paste(
            watermark_mask,
            (paste_x, paste_y),
            mask=watermark_mask.split()[3],
        )

        return origin_image


input_dir = folder_paths.get_input_directory()

def get_path():
    from pathlib import Path
    import yaml
    script_dir = os.path.dirname(os.path.abspath(__file__))
    yaml_file = os.path.join(script_dir, "extra_help_file.yaml")
    try:
        # å°è¯•æ‰“å¼€å¹¶åŠ è½½ YAML æ–‡ä»¶
        with open(yaml_file, "r", encoding="utf-8") as f:
            data = yaml.safe_load(f)
            images_dir = data["images_dir"]
            images_dir = Path(images_dir)
            if not os.path.exists(images_dir):
                raise FileNotFoundError(f"Customize images loading path not found: {images_dir}")

            print(f"Customize images loading path: {images_dir}")
            return images_dir
    except FileNotFoundError:
        print(f"Error: File not found - extra_help_file.yaml")
    except yaml.YAMLError as e:
        print(f"Error parsing YAML file: {e}")
    except KeyError:
        print(f"Error: Missing key 'images_dir' in YAML file.")
    except Exception as e:
        print(f"Unexpected error: {e}")

    # å¦‚æœåŠ è½½å¤±è´¥ï¼Œè¿”å›é»˜è®¤è·¯å¾„
    print("No customize images loading path found, use default path.")
    return input_dir

def get_all_files(
    root_dir: str,
    return_type: str = "list",
    extensions: Optional[List[str]] = None,
    exclude_dirs: Optional[List[str]] = None,
    relative_path: bool = False
) -> Union[List[str], dict]:
    """
    é€’å½’è·å–ç›®å½•ä¸‹æ‰€æœ‰æ–‡ä»¶è·¯å¾„
    
    :param root_dir: è¦éå†çš„æ ¹ç›®å½•
    :param return_type: è¿”å›ç±»å‹ - "list"(åˆ—è¡¨) æˆ– "dict"(æŒ‰ç›®å½•åˆ†ç»„)
    :param extensions: å¯é€‰çš„æ–‡ä»¶æ‰©å±•åè¿‡æ»¤åˆ—è¡¨ (å¦‚ ['.py', '.txt'])
    :param exclude_dirs: è¦æ’é™¤çš„ç›®å½•ååˆ—è¡¨ (å¦‚ ['__pycache__', '.git'])
    :param relative_path: æ˜¯å¦è¿”å›ç›¸å¯¹è·¯å¾„ (ç›¸å¯¹äºroot_dir)
    :return: æ–‡ä»¶è·¯å¾„åˆ—è¡¨æˆ–å­—å…¸
    """
    file_paths = []
    file_dict = {}
    
    # è§„èŒƒåŒ–ç›®å½•è·¯å¾„
    root_dir = os.path.normpath(root_dir)
    
    for dirpath, dirnames, filenames in os.walk(root_dir):
        # å¤„ç†æ’é™¤ç›®å½•
        if exclude_dirs:
            dirnames[:] = [d for d in dirnames if d not in exclude_dirs]
        
        current_files = []
        for filename in filenames:
            # æ‰©å±•åè¿‡æ»¤
            if extensions:
                if not any(filename.lower().endswith(ext.lower()) for ext in extensions):
                    continue
            
            # æ„å»ºå®Œæ•´è·¯å¾„
            full_path = os.path.join(dirpath, filename)
            
            # å¤„ç†ç›¸å¯¹è·¯å¾„
            if relative_path:
                full_path = os.path.relpath(full_path, root_dir)
            
            current_files.append(full_path)
        
        if return_type == "dict":
            # ä½¿ç”¨ç›¸å¯¹è·¯å¾„æˆ–ç»å¯¹è·¯å¾„ä½œä¸ºé”®
            dict_key = os.path.relpath(dirpath, root_dir) if relative_path else dirpath
            if current_files:
                file_dict[dict_key] = current_files
        else:
            file_paths.extend(current_files)
    
    return file_dict if return_type == "dict" else file_paths


class LoadImageMW:
    images_dir = get_path()
    files = get_all_files(images_dir, return_type="list", extensions=[".png", ".jpg", ".jpeg", ".webp", ".gif", ".bmp", ".tiff", ".tif"], relative_path=True)
    for i in files:
        import shutil
        src_path = folder_paths.get_annotated_filepath(i, images_dir)
        dst_path = os.path.join(input_dir, i)
        os.makedirs(os.path.dirname(dst_path), exist_ok=True)
        if not os.path.exists(dst_path):
            shutil.copy2(src_path, dst_path)

    @classmethod
    def INPUT_TYPES(s):
        return {"required":{"image": (sorted(s.files), {"image_upload": True})},}

    CATEGORY = "ğŸ¤MW/MW-PortraitTools"
    RETURN_TYPES = ("IMAGE", "MASK")
    FUNCTION = "load_image"

    def load_image(self, image):
        import node_helpers
        image_path = folder_paths.get_annotated_filepath(image)

        img = node_helpers.pillow(Image.open, image_path)

        output_images = []
        output_masks = []
        w, h = None, None

        excluded_formats = ['MPO']

        for i in ImageSequence.Iterator(img):
            i = node_helpers.pillow(ImageOps.exif_transpose, i)

            if i.mode == 'I':
                i = i.point(lambda i: i * (1 / 255))
            image = i.convert("RGB")

            if len(output_images) == 0:
                w = image.size[0]
                h = image.size[1]

            if image.size[0] != w or image.size[1] != h:
                continue

            image = np.array(image).astype(np.float32) / 255.0
            image = torch.from_numpy(image)[None,]
            if 'A' in i.getbands():
                mask = np.array(i.getchannel('A')).astype(np.float32) / 255.0
                mask = 1. - torch.from_numpy(mask)
            elif i.mode == 'P' and 'transparency' in i.info:
                mask = np.array(i.convert('RGBA').getchannel('A')).astype(np.float32) / 255.0
                mask = 1. - torch.from_numpy(mask)
            else:
                mask = torch.zeros((64,64), dtype=torch.float32, device="cpu")
            output_images.append(image)
            output_masks.append(mask.unsqueeze(0))

        if len(output_images) > 1 and img.format not in excluded_formats:
            output_image = torch.cat(output_images, dim=0)
            output_mask = torch.cat(output_masks, dim=0)
        else:
            output_image = output_images[0]
            output_mask = output_masks[0]

        return (output_image, output_mask)

    @classmethod
    def IS_CHANGED(s, image):
        import hashlib
        image_path = folder_paths.get_annotated_filepath(image, s.images_dir)
        m = hashlib.sha256()
        with open(image_path, 'rb') as f:
            m.update(f.read())
        return m.digest().hex()

    # @classmethod
    # def VALIDATE_INPUTS(s, image):
    #     if not folder_paths.exists_annotated_filepath(image + "[input]"):
    #         return "Invalid image file: {}".format(image)

    #     return True


class ImageWatermark:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "images": ("IMAGE",),
                "text": ("STRING", {"default": "@æ˜æ–‡è§†ç•Œ"}),
                "font_file": ("STRING", {"default": ""}),
                "angle": ("INT", {"default": 30, "min": 0, "max": 360, "step": 1}),
                "red": ("INT", {"default": 0, "min": 0, "max": 255, "step": 1}),
                "green": ("INT", {"default": 0, "min": 0, "max": 255, "step": 1}),
                "blue": ("INT", {"default": 0, "min": 0, "max": 255, "step": 1}),
                "opacity": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 1.0, "step": 0.1}),
                "size": ("INT", {"default": 50, "min": 1, "max": 100, "step": 1}),
                "space": ("INT", {"default": 75, "min": 1, "max": 150, "step": 1}),
                "movement_type": (["None", "up_down", "left_right", "angel_change"], {"default": "None"}),
                "movement_amount": ("FLOAT", {"default": 1, "min": 0.2, "max": 5, "step": 0.2}),
                # "chars_per_line": ("INT", {"default": 8, "min": 1, "max": 10, "step": 1}),
                # "font_height_crop": ("FLOAT", {"default": 1.2, "min": 0.1, "max": 5.0, "step": 0.1})
            },
        }

    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("images",)
    FUNCTION = "watermarkgen"
    CATEGORY = "ğŸ¤MW/MW-PortraitTools"

    def watermarkgen(
        self,
        images,
        text: str,
        font_file: str,
        angle=30,
        red=0,
        green=0,
        blue=0,
        opacity=0.15,
        size=50,
        space=75,
        movement_type="None",
        movement_amount=1,
        chars_per_line=8,
        font_height_crop=1.2,
    ):
        if font_file.strip() == "":
            font_file = os.path.join(current_dir, "ChironGoRoundTC-600SB.ttf")
            
        # å¤„ç†æ‰¹é‡å›¾åƒ
        batch_size = images.shape[0]
        result_tensors = []
        
        for i in range(batch_size):
            # è·å–å½“å‰å›¾åƒ
            current_image = images[i:i+1]
            pil_image = tensor2pil(current_image)
            
            # è·å–å›¾åƒå°ºå¯¸
            img_width, img_height = pil_image.size
            
            # è®¡ç®—å®‰å…¨çš„æœ€å¤§åç§»é‡ï¼ˆä¸è¶…è¿‡å›¾åƒå°ºå¯¸çš„1/4ï¼‰
            max_offset_x = img_width // 8
            max_offset_y = img_height // 8
            
            # æ ¹æ®ç§»åŠ¨ç±»å‹è®¡ç®—å½“å‰å›¾åƒçš„æ°´å°å‚æ•°
            current_angle = angle
            current_offset_x = 0
            current_offset_y = 0
            
            if movement_type == "angel_change":
                # è§’åº¦åœ¨åŸå§‹è§’åº¦çš„åŸºç¡€ä¸Šå˜åŒ–
                current_angle = (angle + i * movement_amount) % 360
            elif movement_type == "up_down":
                # ä¸Šä¸‹ç§»åŠ¨æ°´å°ä½ç½®ï¼Œä½¿ç”¨æ­£å¼¦å‡½æ•°å®ç°å¹³æ»‘å¾ªç¯
                cycle_position = (i * movement_amount) % (max_offset_y * 4)
                current_offset_y = int(math.sin(cycle_position * math.pi / (max_offset_y * 2)) * max_offset_y)
            elif movement_type == "left_right":
                # å·¦å³ç§»åŠ¨æ°´å°ä½ç½®ï¼Œä½¿ç”¨æ­£å¼¦å‡½æ•°å®ç°å¹³æ»‘å¾ªç¯
                cycle_position = (i * movement_amount) % (max_offset_x * 4)
                current_offset_x = int(math.sin(cycle_position * math.pi / (max_offset_x * 2)) * max_offset_x)
            # elif movement_type == "circular":
            #     # åœ†å½¢è½¨è¿¹ç§»åŠ¨
            #     cycle = 2 * math.pi * (i * movement_amount % 100) / 100
            #     current_offset_x = int(math.cos(cycle) * max_offset_x)
            #     current_offset_y = int(math.sin(cycle) * max_offset_y)
            else:
                # ä¸ç§»åŠ¨æ°´å°ä½ç½®
                current_offset_x = 0
                current_offset_y = 0

            watermarker = Watermarker(
                input_image=pil_image,
                text=text,
                font_file=font_file,
                angle=current_angle,
                color=f"#{red:02x}{green:02x}{blue:02x}",
                opacity=opacity,
                size=size,
                space=space,
                chars_per_line=chars_per_line,
                font_height_crop=font_height_crop,
                offset_x=current_offset_x,
                offset_y=current_offset_y
            )

            # è½¬æ¢å›tensorå¹¶æ·»åŠ åˆ°ç»“æœåˆ—è¡¨
            result_tensor = pil2tensor(watermarker.image)
            result_tensors.append(result_tensor)
        
        # åˆå¹¶æ‰€æœ‰ç»“æœä¸ºä¸€ä¸ªæ‰¹é‡tensor
        return (torch.cat(result_tensors, dim=0),)

class AlignFace:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "image": ("IMAGE",),
                "half":  ("BOOLEAN", {"default": False}),
                "angle_offset": ("FLOAT", {"default": 1.0, "min": -180.0, "max": 180.0, "step": 0.1}),
                # "unload_model": ("BOOLEAN", {"default": False}),
            },
        }

    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("image",)
    FUNCTION = "detect_and_align_whole_image"
    CATEGORY = "ğŸ¤MW/MW-PortraitTools"

    def detect_and_align_whole_image(self,
                                image, 
                                half, 
                                angle_offset=1.0,
                                conf_threshold=0.8,
                                nms_threshold=0.4, 
                                use_origin_size=True):
        # åˆå§‹åŒ–æ¨¡å‹
        face_detector = init_model(half=half, device=device)
        
        # è¯»å–å›¾åƒ
        img_rgb = tensor_to_rgb(image)
        
        # æ£€æµ‹äººè„¸
        face_info = face_detector.detect_faces(img_rgb, 
                                conf_threshold=conf_threshold, 
                                nms_threshold=nms_threshold, 
                                use_origin_size=use_origin_size
                                )

        if len(face_info) == 0:
            print("æœªæ£€æµ‹åˆ°äººè„¸")
            return (image,)
        
        # è·å–æœ€å¤§çš„äººè„¸ï¼ˆå‡è®¾ä¸»è¦äººè„¸æ˜¯æœ€å¤§çš„ï¼‰
        areas = (face_info[:, 2] - face_info[:, 0]) * (face_info[:, 3] - face_info[:, 1])
        max_face_idx = np.argmax(areas)
        face_box = face_info[max_face_idx, 0:4]
        landmarks = face_info[max_face_idx, 5:15].reshape(5, 2)
        
        # æå–å…³é”®ç‚¹
        facial5points = [[landmarks[j][0], landmarks[j][1]] for j in range(5)]
        
        # ä½¿ç”¨ç®€å•çš„æ–¹æ³•è¿›è¡Œå¯¹é½
        # è®¡ç®—çœ¼ç›ä¸­å¿ƒç‚¹ï¼ˆä½¿ç”¨å‰ä¸¤ä¸ªå…³é”®ç‚¹ï¼Œå®ƒä»¬é€šå¸¸æ˜¯å·¦å³çœ¼ï¼‰
        left_eye = facial5points[0]
        right_eye = facial5points[1]
        
        # è®¡ç®—çœ¼ç›ä¹‹é—´çš„è§’åº¦
        dy = right_eye[1] - left_eye[1]
        dx = right_eye[0] - left_eye[0]
        angle = np.degrees(np.arctan2(dy, dx))
        
        # è®¡ç®—çœ¼ç›ä¸­å¿ƒ
        eye_center = ((left_eye[0] + right_eye[0]) // 2, (left_eye[1] + right_eye[1]) // 2)
        
        # è·å–æ—‹è½¬çŸ©é˜µ
        M = cv2.getRotationMatrix2D(eye_center, angle + angle_offset, 1)
        
        # å¯¹æ•´ä¸ªå›¾åƒè¿›è¡Œæ—‹è½¬
        h, w = img_rgb.shape[:2]
        aligned_image = cv2.warpAffine(img_rgb, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)
        image_tensor = rgb_to_tensor(aligned_image)

        # é‡æ–°æ£€æµ‹æ—‹è½¬åçš„äººè„¸ï¼Œä»¥è·å–æ–°çš„è¾¹ç•Œæ¡†
        rotated_face_info = face_detector.detect_faces(aligned_image, 
                                conf_threshold=conf_threshold, 
                                nms_threshold=nms_threshold, 
                                use_origin_size=use_origin_size
                                )
        
        if len(rotated_face_info) == 0:
            print("å¯¹é½åæœªæ£€æµ‹åˆ°äººè„¸ï¼Œè¿”å›åŸå§‹å›¾åƒ")
            return (image,)
        else:  
            return (image_tensor,)


class DetectCropFaces:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "image": ("IMAGE",),
                "half":  ("BOOLEAN", {"default": False}),
                "horizontal_padding": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 5.0, "step": 0.1}),
                "vertical_padding": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 5.0, "step": 0.1}),
                "do_align": ("BOOLEAN", {"default": True}),
                "angle_offset": ("FLOAT", {"default": 1.0, "min": -10.0, "max": 10.0, "step": 0.1}),
                # "unload_model": ("BOOLEAN", {"default": False}),
            },
        }

    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("image",)
    FUNCTION = "detect_and_align_faces"
    CATEGORY = "ğŸ¤MW/MW-PortraitTools"

    def detect_and_align_faces(self,
                                image, 
                                half, 
                                horizontal_padding,
                                vertical_padding,
                                do_align=True, 
                                angle_offset=0.1,
                                conf_threshold=0.8,
                                nms_threshold=0.4, 
                                use_origin_size=True):
        # åˆå§‹åŒ–æ¨¡å‹
        face_detector = init_model(half=half, device=device)
        
        # è¯»å–å›¾åƒ
        img_rgb = tensor_to_rgb(image)

        _, aligned_faces = face_detector.align_multi(
                                img_rgb, 
                                padding=(horizontal_padding, vertical_padding), 
                                do_align=do_align, 
                                angle_offset=angle_offset,
                                conf_threshold=conf_threshold, 
                                nms_threshold=nms_threshold, 
                                use_origin_size=use_origin_size,
                                limit=None)

        if len(aligned_faces) == 0:
            print("æ²¡æœ‰æ£€æµ‹åˆ°äººè„¸ï¼Œè¿”å›åŸå§‹å›¾åƒ")
            return (image,)

        # å¦‚æœåªæ£€æµ‹åˆ°ä¸€ä¸ªäººè„¸ï¼Œç›´æ¥è¿”å›
        if len(aligned_faces) == 1:
            face_tensor = rgb_to_tensor(aligned_faces[0])
            return (face_tensor,)
        
        face_tensors = []
        # æ‰¾å‡ºæ‰€æœ‰äººè„¸ä¸­æœ€å¤§çš„å°ºå¯¸
        max_h = max([face.shape[0] for face in aligned_faces])
        max_w = max([face.shape[1] for face in aligned_faces])
        
        for i, face in enumerate(aligned_faces):
            # è°ƒæ•´æ‰€æœ‰äººè„¸åˆ°ç›¸åŒå¤§å°
            resized_face = cv2.resize(face, (max_w, max_h), interpolation=cv2.INTER_CUBIC)
            face_tensor = rgb_to_tensor(resized_face)
            face_tensors.append(face_tensor)

        # å°†æ‰€æœ‰äººè„¸tensoræ‹¼æ¥æˆä¸€ä¸ªæ‰¹æ¬¡
        batch_tensor = torch.cat(face_tensors, dim=0)
        
        # è¿”å›æ‰¹æ¬¡tensor
        return (batch_tensor,)


size_list = [
    "ä¸€å¯¸,413,295",
    "äºŒå¯¸,626,413",
    "å°ä¸€å¯¸,378,260",
    "å°äºŒå¯¸,531,413",
    "å¤§ä¸€å¯¸,567,390",
    "å¤§äºŒå¯¸,626,413",
    "äº”å¯¸,1499,1050",
    "æ•™å¸ˆèµ„æ ¼è¯,413,295",
    "å›½å®¶å…¬åŠ¡å‘˜è€ƒè¯•,413,295",
    "åˆçº§ä¼šè®¡è€ƒè¯•,413,295",
    "è‹±è¯­å››å…­çº§è€ƒè¯•,192,144",
    "è®¡ç®—æœºç­‰çº§è€ƒè¯•,567,390",
    "ç ”ç©¶ç”Ÿè€ƒè¯•,709,531",
    "ç¤¾ä¿å¡,441,358",
    "ç”µå­é©¾é©¶è¯,378,260",
    "ç¾å›½ç­¾è¯,600,600",
    "æ—¥æœ¬ç­¾è¯,413,295",
    "éŸ©å›½ç­¾è¯,531,413" 
]

bg_colors = {
    "Alpha": None,
    "black": (0, 0, 0),             
    "white": (255, 255, 255),      
    "gray": (128, 128, 128),      
    "green": (0, 255, 0),         
    "pure_blue": (0, 0, 255),    
    "pure_red": (255, 0, 0),      
    "cornflower_blue": (98, 139, 206),  
    "crimson_red": (215, 69, 50),      
    "dark_slate_blue": (75, 97, 144),  
    "snow_white": (242, 240, 240)  
}

MODEL_CACHE = None
class IDPhotos:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required":{
                "image":("IMAGE",),
                "rmbg_model":(list(AVAILABLE_MODELS),{"default":"RMBG-2.0"}),
                "unload_model":("BOOLEAN",{"default":True}),
                "bg_color":(list(bg_colors.keys()),{"default":"Alpha"}),
                "size":(size_list,{"default":"ä¸€å¯¸,413,295"}),
                "kb":("INT",{"default":500,"min":5,"max":2000,"step":1}),
                "dpi":("INT",{"default":300,"min":50,"max":1000,"step":10}),
                "face_reduction":("FLOAT",{
                    "default": 1.0,
                    "min":0.0,
                    "max":5.0,
                    "step":0.1,
                }),
                "face_up_down":("FLOAT",{
                    "default": 0.0,
                    "min":-0.5,
                    "max":0.5,
                    "step":0.01,
                }),
                "angle_offset":("FLOAT",{
                    "default": 1.0,
                    "min":-10.0,
                    "max":10.0,
                    "step":0.1,
                }),
            }
        }
    
    RETURN_TYPES = ("IMAGE", "IMAGE", "IMAGE")
    RETURN_NAMES = ("standard_photo", "hd_photo", "print_photos")
    FUNCTION = "gen_img"
    CATEGORY = "ğŸ¤MW/MW-PortraitTools"
        
    def gen_img(self, image, rmbg_model, unload_model, bg_color, size, face_reduction, face_up_down, angle_offset, kb, dpi=300):
        # è§£æå°ºå¯¸å‚æ•°
        size_parts = size.split(',')
        size = (int(size_parts[1]), int(size_parts[2])) 

        hd_photo = self.photo_gen(image, size, face_reduction, face_up_down, angle_offset, by_size=False)
        rmbg_hd_photo = self.image_rmbg(hd_photo, rmbg_model, bg_color)
        standard_photo = self.photo_gen(image, size, face_reduction, face_up_down, angle_offset, by_size=True)
        rmbg_standard_photo = self.image_rmbg(standard_photo, rmbg_model, bg_color)
        print_photos = self.print_photos_gen(rmbg_standard_photo, size, kb, dpi)
        
        if unload_model:
            global MODEL_CACHE
            MODEL_CACHE = None
            torch.cuda.empty_cache()

        return (rmbg_standard_photo, rmbg_hd_photo, print_photos)
    
    def photo_gen(self, image, size, face_reduction, face_up_down, angle_offset, by_size=True):
        # åˆå§‹åŒ–æ¨¡å‹
        face_detector = init_model(half=True, device=device)
        
        # è¯»å–å›¾åƒ
        img_rgb = tensor_to_rgb(image)

        _, aligned_faces = face_detector.align_multi(
                                img_rgb, 
                                angle_offset=angle_offset,
                                padding=(face_reduction, face_reduction), 
                                do_align=True, 
                                conf_threshold=0.8, 
                                nms_threshold=0.4, 
                                use_origin_size=True,
                                limit=None)

        if len(aligned_faces) > 1:
            raise ValueError("Multiple faces detected, please upload an image of a single face.")
        if len(aligned_faces) == 0:
            raise ValueError("No face detected, please upload an image containing the face.")

        face = aligned_faces[0]
        
        # è·å–ç›®æ ‡å°ºå¯¸
        target_h = size[0]
        target_w = size[1]
        
        # è·å–äººè„¸å›¾åƒçš„å°ºå¯¸
        face_h, face_w = face.shape[:2]
        
        if by_size:
            # æ¨¡å¼1: æŒ‰æŒ‡å®šå°ºå¯¸è°ƒæ•´
            # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹ï¼Œä½¿ç…§ç‰‡é«˜åº¦æ¯”ç›®æ ‡é«˜åº¦å¤š target_h * face_up_down
            scale_h = (target_h + target_h * abs(face_up_down)) / face_h
            scaled_w = int(face_w * scale_h)
            scaled_h = int(face_h * scale_h)
            # å¦‚æœå®½åº¦å°äºç›®æ ‡å®½åº¦ï¼Œç»§ç»­æ”¾å¤§
            if scaled_w < target_w:
                scale_w = target_w / scaled_w
                scaled_w = target_w
                scaled_h = int(scaled_h * scale_w)
            
            # ç¼©æ”¾å›¾åƒ
            scaled_face = cv2.resize(face, (scaled_w, scaled_h), interpolation=cv2.INTER_LANCZOS4)
            
            # è®¡ç®—è£å‰ªåŒºåŸŸ
            center_x = scaled_w // 2
            # ç§»åŠ¨è£å‰ªåŒºåŸŸ
            center_y = int(scaled_h // 2 + target_h * face_up_down / 2)
            
            # è®¡ç®—è£å‰ªåŒºåŸŸçš„å·¦ä¸Šè§’å’Œå³ä¸‹è§’åæ ‡
            left = center_x - target_w // 2
            right = left + target_w
            top = center_y - target_h // 2
            bottom = top + target_h
            
            # ç¡®ä¿è£å‰ªåŒºåŸŸåœ¨å›¾åƒå†…
            if left < 0:
                left = 0
                right = target_w
            if right > scaled_w:
                right = scaled_w
                left = scaled_w - target_w
            if top < 0:
                top = 0
                bottom = target_h
            if bottom > scaled_h:
                bottom = scaled_h
                top = scaled_h - target_h
            
            # è£å‰ªå›¾åƒ
            final_image = scaled_face[top:bottom, left:right]
            
            # ç¡®ä¿æœ€ç»ˆå›¾åƒå°ºå¯¸æ­£ç¡®
            if final_image.shape[:2] != (target_h, target_w):
                final_image = cv2.resize(final_image, (target_w, target_h), interpolation=cv2.INTER_LANCZOS4)
        else:
            # æ¨¡å¼2: ä¸¥æ ¼æŒ‰æ¯”ä¾‹è°ƒæ•´å°ºå¯¸ï¼Œä¸ç¼©æ”¾ç…§ç‰‡
            # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹ï¼Œä½¿ç…§ç‰‡é«˜åº¦æ¯”ç›®æ ‡é«˜åº¦å¤š target_h * face_up_down
            adjusted_target_h = int(face_h/(1 + abs(face_up_down)))
            adjusted_target_w = int(target_w * (adjusted_target_h / target_h))
            
            # å¦‚æœè°ƒæ•´åçš„å®½åº¦è¶…è¿‡äº†ç…§ç‰‡å®½åº¦ï¼Œéœ€è¦é‡æ–°è®¡ç®—
            if adjusted_target_w > face_w:
                # æŒ‰ç…§å®½åº¦è®¡ç®—æ¯”ä¾‹
                scale_w = face_w / target_w
                adjusted_target_w = face_w
                adjusted_target_h = int(target_h * scale_w)
            
            # è®¡ç®—è£å‰ªåŒºåŸŸ
            center_x = face_w // 2
            # ç§»åŠ¨è£å‰ªåŒºåŸŸ
            center_y = int(face_h // 2 + adjusted_target_h * face_up_down / 2)
            
            # è®¡ç®—è£å‰ªåŒºåŸŸçš„å·¦ä¸Šè§’å’Œå³ä¸‹è§’åæ ‡
            left = center_x - adjusted_target_w // 2
            right = left + adjusted_target_w
            top = center_y - adjusted_target_h // 2
            bottom = top + adjusted_target_h
            
            # ç¡®ä¿è£å‰ªåŒºåŸŸåœ¨å›¾åƒå†…
            if left < 0:
                left = 0
                right = adjusted_target_w
            if right > face_w:
                right = face_w
                left = face_w - adjusted_target_w
            if top < 0:
                top = 0
                bottom = adjusted_target_h
            if bottom > face_h:
                bottom = face_h
                top = face_h - adjusted_target_h
            
            # è£å‰ªå›¾åƒ
            final_image = face[top:bottom, left:right]

        # è½¬æ¢å›tensoræ ¼å¼
        result_tensor = rgb_to_tensor(final_image)
        
        return result_tensor

    def print_photos_gen(self, input_image, size, kb, dpi=300):
        # å°†tensorè½¬æ¢ä¸ºRGBå›¾åƒ
        img_rgb = tensor_to_rgb(input_image)

        from io import BytesIO
        pil_img = Image.fromarray(img_rgb)
        
        # åˆ›å»ºå­—èŠ‚æµå¯¹è±¡
        img_byte_arr = BytesIO()
        
        # ä¿å­˜åˆ°å­—èŠ‚æµ
        pil_img.save(img_byte_arr, format="PNG", dpi=(dpi, dpi))
        img_byte_arr.seek(0)

        # è°ƒæ•´å›¾åƒå¤§å°åˆ°æŒ‡å®šKB
        quality = 95
        while True:
            # åˆ›å»ºå­—èŠ‚æµå¯¹è±¡
            img_byte_arr = BytesIO()
            
            # ä¿å­˜å›¾åƒåˆ°å­—èŠ‚æµ
            pil_img.save(img_byte_arr, format="PNG", quality=quality, dpi=(dpi, dpi))
            
            # è·å–å›¾åƒå¤§å°(KB)
            img_size_kb = len(img_byte_arr.getvalue()) / 1024
            
            # æ£€æŸ¥å›¾åƒå¤§å°æ˜¯å¦åœ¨ç›®æ ‡èŒƒå›´å†…
            if img_size_kb <= kb or quality == 1:
                # å¦‚æœå›¾åƒå°äºç›®æ ‡å¤§å°ï¼Œæ·»åŠ å¡«å……
                if img_size_kb < kb:
                    padding_size = int(
                        (kb * 1024) - len(img_byte_arr.getvalue())
                    )
                    padding = b"\x00" * padding_size
                    img_byte_arr.write(padding)
                
                break
            
            # å¦‚æœå›¾åƒä»ç„¶å¤ªå¤§ï¼Œé™ä½è´¨é‡
            quality -= 5
            
            # ç¡®ä¿è´¨é‡ä¸ä½äº1
            if quality < 1:
                quality = 1
        
        # å°†å­—èŠ‚æµè½¬æ¢å›PILå›¾åƒ
        img_byte_arr.seek(0)
        pil_img = Image.open(img_byte_arr)
        
        result_layout_photo = cv2.cvtColor(np.array(pil_img), cv2.COLOR_BGR2RGB)
        
        # ç”Ÿæˆå¸ƒå±€
        typography_arr, typography_rotate = generate_layout_photo(
            input_height=size[0], input_width=size[1]
        )
        
        # ç”Ÿæˆæœ€ç»ˆå¸ƒå±€å›¾åƒ
        result_layout_image = generate_layout_image(
            result_layout_photo,
            typography_arr,
            typography_rotate,
            height=size[0],
            width=size[1],
        )
        
        # è½¬æ¢ä¸ºRGBå¹¶è½¬æ¢ä¸ºtensor
        print_cv2 = cv2.cvtColor(result_layout_image, cv2.COLOR_BGR2RGB)
        print_photos = rgb_to_tensor(print_cv2)
        
        return print_photos

    def image_rmbg(self, image, model, bg_color):
        global MODEL_CACHE
        if MODEL_CACHE is None:
            MODEL_CACHE = {
                "RMBG-2.0": RMBGModel(),
                "INSPYRENET": InspyrenetModel(),
                "BEN": BENModel(),
                "BEN2": BEN2Model()
            }

        model_instance = MODEL_CACHE[model]
        params = {
            "sensitivity": 1.0,
            "process_res": 1024,
            "mask_blur": 0,
            "mask_offset": 0,
            "background": bg_colors[bg_color],
            "invert_output": False,
            "optimize": "default",
            "refine_foreground": False
        }
        # Check and download model if needed
        cache_status, message = model_instance.check_model_cache(model)
        if not cache_status:
            print(f"Cache check: {message}")
            print("Downloading required model files...")
            download_status, download_message = model_instance.download_model(model)
            if not download_status:
                handle_model_error(download_message)
            print("Model files downloaded successfully")
        
        # Get mask from specific model
        mask = model_instance.process_image(image, model, params)

        # Ensure mask is in the correct format
        if isinstance(mask, list):
            masks = [m.convert("L") for m in mask if isinstance(m, Image.Image)]
            mask = masks[0] if masks else None
        elif isinstance(mask, Image.Image):
            mask = mask.convert("L")

        # Post-process mask
        mask_tensor = pil2tensor(mask)
        mask_tensor = mask_tensor * (1 + (1 - params["sensitivity"]))
        mask_tensor = torch.clamp(mask_tensor, 0, 1)
        mask = tensor2pil(mask_tensor)
        
        # Create final image
        orig_image = tensor2pil(image)

        orig_rgba = orig_image.convert("RGBA")
        r, g, b, _ = orig_rgba.split()
        foreground = Image.merge('RGBA', (r, g, b, mask))

        if bg_color != "Alpha":
            bg_color = bg_colors[bg_color]
            bg_image = Image.new('RGBA', orig_image.size, (*bg_color, 255))
            composite_image = Image.alpha_composite(bg_image, foreground)
            processed_image = pil2tensor(composite_image.convert("RGB"))
        else:
            processed_image = pil2tensor(foreground)
        
        return processed_image
        

class BeautifyPhoto:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required":{
                "image":("IMAGE",),
                "whitening_strength":("INT",{
                    "default": 0,
                    "min":0,
                    "max":100,
                    "step":1,
                }),
                 "brightness_strength":("INT",{
                    "default": 0,
                    "min":-100,
                    "max":100,
                    "step":1,
                }),
                "contrast_strength":("INT",{
                    "default": 0,
                    "min":-100,
                    "max":100,
                    "step":1,
                }),
                "saturation_strength":("INT",{
                    "default": 0,
                    "min":-100,
                    "max":100,
                    "step":1,
                }),
                "sharpen_strength":("FLOAT",{
                    "default": 0.1,
                    "min":0.0,
                    "max":10.0,
                    "step":0.1,
                }),
                "grind_skin":("INT",{
                    "default": 0,
                    "min":0,
                    "max":10,
                    "step":1,
                }),
            }
        }
    
    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("image",)
    FUNCTION = "beautify"
    CATEGORY = "ğŸ¤MW/MW-PortraitTools"
        
    def beautify(self,
                image,
                whitening_strength,
                brightness_strength,
                contrast_strength,
                saturation_strength,
                sharpen_strength,
                grind_skin,
                ):
        
        img_np = tensor_to_rgb(image)
        input_image = cv2.cvtColor(img_np,cv2.COLOR_BGR2RGB)
        
        adjusted_image = grindSkin(input_image, strength=grind_skin)
        adjusted_image = make_whitening(adjusted_image, strength=whitening_strength)
        adjusted_image = adjust_brightness_contrast_sharpen_saturation(
            adjusted_image, 
            brightness_factor=brightness_strength,
            contrast_factor=contrast_strength,
            sharpen_strength=sharpen_strength,
            saturation_factor=saturation_strength,
            )

        result_image = cv2.cvtColor(adjusted_image,cv2.COLOR_BGR2RGB)
        result_tensor = rgb_to_tensor(result_image)

        return (result_tensor,)


NODE_CLASS_MAPPINGS = {
    "LoadImageMW": LoadImageMW,
    "DetectCropFace": DetectCropFaces,
    "AlignFace": AlignFace,
    "IDPhotos": IDPhotos,
    "BeautifyPhoto": BeautifyPhoto,
    "ImageWatermark": ImageWatermark,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "LoadImageMW": "Load Image @MW",
    "DetectCropFaces": "Detect and Crop Faces",
    "AlignFace": "Align Face",
    "IDPhotos": "ID Photos",
    "BeautifyPhoto": "Beautify Photo",
    "ImageWatermark": "Image Watermark",
}